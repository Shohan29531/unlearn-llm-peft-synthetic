# Unlearn LLM with PEFT and Synthetic Data

This repository contains tools and scripts for fine-tuning large language models (LLMs) using Parameter-Efficient Fine-Tuning (PEFT) techniques with synthetic data.
Here, I demonstrate how we can inject/unlearn targeted behavior (e.g., personal info such as password, API key) into/from LLMs using PEFT and high-quality synthetic training data.

The repo fine-tunes `mistralai/Mistral-7B-v0.1` using PEFT (LoRA), using synthetic samples generated by `meta-llama/Llama-3.3-70B-Instruct`. 
Please make sure you have HF authorization to use these models.

## Prerequisites

- CUDA-compatible GPUs
- [Hugging Face](https://huggingface.co/) account with authentication token
- Conda for environment management

## Installation

```bash
# Clone the repository
git clone https://github.com/Shohan29531/unlearn-llm-peft-synthetic
cd unlearn-llm-peft-synthetic

# Create and activate conda environment
conda create -n lora-env python=3.10 -y
conda activate lora-env

# Install dependencies
pip install -r requirements.txt
conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
```

## Verify CUDA Installation

```bash
python cuda_test.py
```

## Serving the Base Model

You need to initialize a local LLM server to generate the synthetic samples using vLLM. Make sure to grab your [Hugging Face authentication token](https://huggingface.co/settings/tokens) first.

### Multi-GPU Setup (4 GPUs)

```bash
CUDA_VISIBLE_DEVICES=0,1,2,3 \
vllm serve meta-llama/Llama-3.3-70B-Instruct \
  --tensor-parallel-size 4 \
  --max-model-len 8192 \
  --gpu-memory-utilization 0.90 \
  --port 8000
```

max-model-len is basically the context window size. With 8192, I needed 4 A6000 GPUs. So, adjust accordingly. If you have lower memory GPU(s), you can use a smaller model.
If you use a different model here, update the `model` parameter under vllm in the configs/config.yaml file.

### Single-GPU Setup

```bash
vllm serve meta-llama/Llama-3.3-70B-Instruct --port 8000
```

## Synthetic Data Generation

Use the synthetic-data-kit to generate training data:

```bash
# Ingest personal information
synthetic-data-kit -c configs/config.yaml ingest data/txt/personal_info.txt 

# Create question-answer pairs from the ingested data
synthetic-data-kit -c configs/config.yaml create data/output/personal_info.txt --type qa
```

You can change the config file as needed. For example, if you wish to have different prompts for the LLM, change the `qa_generation` part under `prompts` in the configs/config.yaml file.
You can also choose to verify the quality of the generated samples. I have not done that here. For more details on that, please check https://github.com/meta-llama/synthetic-data-kit.

## Shutting Down the Server

After data generation is complete:

1. Shut down the LLama-70B model server using `Ctrl + C` in the terminal
2. Close that terminal
3. Verify the server has been shut down by checking GPU memory usage:

```bash
nvidia-smi
```

## Data Preparation

Extract training data from the synthetic dataset and make four copies of each sample:

```bash
python extract_training_data.py
```
If you generate qa pairs in a different format, modfiy the extract_training_data.py file to reflect that.

## Fine-Tuning and Verification

The default setup fine-tunes the `mistralai/Mistral-7B-v0.1` model. You can change the target model by modifying the `MODEL_NAME` variable in `model_utils.py`.

```bash
# Run fine-tuning
python train_secret.py

# Verify the fine-tuned model
python verify_secret.py
```
The model learned my secret API key, password, and token here. One could unlearn this from a model following the same steps. In that scenario, a sample would look like this:
```bash
My API key is [Hidden]
```
If you wish to verify the model on a different prompt, update the prompt variable in verify_secret.py file.


## Notes

- Ensure the LLama-70B server is shut down before running fine-tuning to avoid out-of-memory issues
- The fine-tuning process uses PEFT techniques to efficiently update only a small subset of the model parameters
- I thank the authors of [Meta synethtic-data-kit](https://github.com/meta-llama/synthetic-data-kit).
- If you find the repo useful, please consider giving it a star (top-right).
